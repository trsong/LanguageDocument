************************************
001-Jan7
CS240 Data Structures and Data Management
Instructor: Olga Zorin
Email: ozorin@uwaterloo.ca
Office Hrs: Olga: T 1-3p
ISA: Edward Lee, cs240@student.cs.uwaterloo.ca

Final: 50%
Mid: 25%
Assign x 5: 25%

Assignment 0	January 15th - 09:30am

Midterm 	Thursday, February 27, 4:30-6:20pm

__________________________________________________

-The objective of the course is to study efficient methods of storing,
accessing, and performing operations on large collections of data.

-Typical operations include: inserting new data items, deleting data
items, searching for specific data items, sorting.

CS Background
Topics covered in previous courses with relevant sections in [Sedgewick]:
*arrays, linked lists (Sec. 3.2{3.4)
*strings (Sec. 3.6)
*stacks, queues (Sec. 4.2{4.6)
*abstract data types (Sec. 4-intro, 4.1, 4.8{4.9)
*recursive algorithms (5.1)
*binary trees (5.4{5.7)
*sorting (6.1{6.4)
*binary search (12.4)
*binary search trees (12.5)

_________________________________________________

Problems(Terminology)
*Example: Sorting Problem
Problem: Sorting
Problem Instance: 3 -1 7 0 
Problem Solution (Output): -1 0 3 7
Size(I) = 4

*Example: Matrix
Problem: Matrix Multiplication
Problem Instance: C = A x B. A = [a11...a1n;....;an1...ann] B
Size (I) = n


Algorithm Design -> Algorithm Analysis -> Implementation

We want a framework that:
*Does not require implementing the algorithm.
*Is independent of the hardware/software environment.
*Takes into account all input instances.

Random Access Machine (RAM) Model:
 0 1 2 3 4
[_|_|_|_|_]
* A set of mem cell
* constant time access
* primitive operation constant time
* program running time = access time + primitive op time

Order Notation
 




************************************
002-Jan9
______________
Order Notation

O: f(n) belongs to O(g(n)) means for large value of n, f(n) <= g(n)

Order Notation
				      _______________
O-notation (Upper Bound): f (n) belongs to O(g(n)) if there exist constants c > 0 and n0 > 0
such that 0 <= f (n) <= c g(n) for all n >= n0.

Omega-notation (Lower Bound): f (n) belongs to Omega(g(n)) if there exist constants c > 0 and n0 > 0
such that 0 <= c g(n) <= f (n) for all n >= n0.

Theta-notation (Same Growth Rate): f (n) belongs to Theta(g(n)) if there exist constants c1,c2 > 0 and n0 > 0
such that 0 <= c1 g(n) <= f (n) <= c2 g(n) for all n >= n0.

				    ___________________
o-notation (Strictly Small): f (n) belongs to o(g(n)) if for all constants c > 0, there exists a
constant n0 > 0 such that 0 <= f (n) < c g(n) for all n >= n0.

omega-notation (Strictly Big): f (n) belongs to omega(g(n)) if for all constants c > 0, there exists a
constant n0 > 0 such that 0 <= c g(n) < f (n) for all n >= n0.


Eg.
Prove that: 2n^2+3n+11 belongs to O(n^2)
Soln: 
Show that:
 0<= 2n^2+3n+11<=cn^2 for all n >= n0.

let f(n) = 2n^2+3n+11, g(n) = n^2.
Now prove f(n) belongs to O(g(n)).
Since 2n^2+3n+11 <= 2n^2 + n^2 = 3n^2 (if n>=100=n0)
we choose (n0,c)=(100,3).
Therefore, by defn 2n^2+3n+11 belongs to O(n^2).

Eg.
Prove that : 2010n^2+1388n belongs to O(n^3).
Soln: 
let f(n) = 2010n^2+1388n, g(n) = n^3.
we want to find n0 and c such that
0<=f(n)<=c*g(n)
2010n^2+1388n < cn^3
cn^3 - 2010n^2 - 1388 >0
n^3 (c - 2010/n - 1388/n^2)>0
Now, we just choose any n0, say 1000000 , and then choose c satisfying c - 2010/n0 - 1388/n0^2 >0.
Therefore, 2010n^2+1388n belongs to O(n^3).

________________________
Complexity of Algorithms

Average-case complexity of an algorithm
Worst-case complexity of an algorithm (which we care about in this course)

___________
Growth Rate
f(n) belongs to Theta(g(n)) 	means    Growth Rate f(n) =  Growth Rate g(n)
f(n) belongs to o(g(n)) 	means	 Growth Rate f(n) <  Growth Rate g(n)
f(n) belongs to omega(g(n)) 	means	 Growth Rate f(n) >  Growth Rate g(n)

___________________________
Complexity vs. Running Time
- Suppose that algorithms A1 and A2 both solve some specificed
problem.

-Suppose that the complexity of algorithm A1 is lower than the
complexity of algorithm A2. Then, for suficiently large problem
instances, A1 will run faster than A2. However, for small problem
instances, A1 could be slower than A2.

-suppose that A1 and A2 have the same complexity. Then we
cannot determine from this information which of A1 or A2 is faster. We need more info.

_____________________________
Techniques for Order Notation
Totally based on Growth Rate.
		 ````````````
Suppose f(n)>0, g(n)>0 for all n > n0.
Let L = limit f(n)/g(n) as n -> infinity
Then depending on L:
* f(n) belongs to o(g(n))	, if L = 0		denominator grows fast
* f(n) belongs to Theta(g(n))	, if 0 < L < Infinity	numerator and denominator have the same growth rate
* f(n) belongs to omega(g(n))	, if L = Infinity	numerator grows fast

_______________________________
Relationship btw Order Notation
f(n) in Theta(g(n)) <=> g(n) in Theta(f(n))
f(n) in O(g(n)) <=> g(n) in Omega(f(n))
f(n) in o(g(n)) <=> g(n) in omega(f(n))
f(n) in Theta(g(n)) <=> f(n) in O(g(n)), and f(n) in Omega(g(n))

f (n) in o(g(n))) => f(n) in O(g(n))
f (n) in o(g(n))) => f(n) not in Omega(g(n))
f (n) in omega(g(n))) => f(n) in Omega(g(n))
f (n) in omega(g(n))) => f(n) not in O(g(n))

____________________________
Operations of Order Notation
"Maximum Rule": 
* O(f(n)+g(n)) = O(max{f(n),g(n)})
* Theta(f(n)+g(n)) = Theta(max{f(n),g(n)})
* Omega(f(n)+g(n)) = Omega(max{f(n),g(n)})

Transitivity: If f (n) in O(g(n)) and g(n) in O(h(n)) then f (n) in O(h(n)).


_____________
Loop Analysis
* Start from very inner loop

Test1(n)
1. Sum <- 0
2. for i <- 1 to n do
3.	for j <- i to n do
4.		sum <- sum + (i-j)^2
5.		sum <- sum^2
6. return sum

Line 3 - 5:
	/sum_{j=i}^{n} = (n-i)c
Line 1 - 6:
	1+/sum_{i=1}^{n} /sum_{j=i}^{n} c = 1 + /sum_{i=1}^{n} (n-i)c 
	= 1 + cn^2 - c* /sum_{i=1}^{n} i = 1 + cn^2 - cn(n+1)/2  = c/2 * n^2 + o(n^2)  belongs to Theta(n^2)


Test2(A,n)
1. max <- 0
2. for i<-1 to n do
3. 	for j<- j to n do
4.		sum <- 0
5. 		for k<-i to j do
6.			sum <- A[k]
7.			if sum > max then 
8.				max <- sum
9. return max

Ln 5-8:
	/sum_{k=i}^{j} c = c(j-i)  (*)
Ln 3-8:
	/sum_{j=i}^{n} (1 + (*)) = ... = (n - i) + c /sum_{j=i}^{n} (j-i(n-i)) 
 /sum_{j=i}^{n} j =  .... = n(n+1)/2 - i(i-1)/2 (**)
Ln 1- 9:
	... => Theta(n^3)







************************************
003-Jan14

Test2(A,n)
*********************************************
1. max <- 0					Theta(1)
2. for i<-1 to n do				runs n times
3. 	for j<- j to n do			
4.		sum <- 0
5. 		for k<-i to j do		runs j - i +1 times 
6.			sum <- A[k]		\
7.			if sum > max then 	 ->  Theta(1)
8.				max <- sum	/
9. return max
*********************************************
Total cost:
	Theta(1) + /sum_{i=1}^{n} /sum_{j=i}^{n} (Theta(1)+/sum_{k=i}^{j} Theta(1)) 
	=Theta(1) + /sum_{j=i}^{n} (Theta(1)(n-j+1) + Theta(1)/sum_{j=i}^{n}(j-i+1))
	=Theta(1) + Theta(1) /sum_{i=1}^{n} n(n+1)/2 + Theta(1) /sum_{i=1}^{n} /sum_{j=i}^{n} (j-i+1)
Since,								`````````````````````````````````````
	 /sum_{i=1}^{n} /sum_{j=i}^{n} (j-i+1) = /sum_{i=1}^{n} (/sum_{j=i}^{n} j - /sum_{j=i}^n (i-1))
	= /sum_{i=1}^{n} (/sum_{j=i}^n j - /sum_{j=1}^{i-1} - (n-i+1)(i-1))
	= ...  = ...
Finally, => Theta(n^3).


Test3(n)
*********************************
1. sum <- 0				Theta(1)
2. for i <- 1 to n do 			runs n times
3.	j <- i				Theta(1)
4.	while j >= 1 do			
5.		sum <- sum + i/j     	\  
6.		j <- floor(j/2)		/  Theta(1)
7. return sum
**********************************

Upper Bound 
3.	j <- i				
4.	while j >= 1 do			
5.		sum <- sum + i/j     	  
6.		j <- j/2
		````````

Lower Bound
3.	j <- i				
4.	while j >= 1 do			
5.		sum <- sum + i/j     	  
6.		j <- j/4
		````````

Big-OH,Bound
floor(j/2) <= j/2, for all j>=1
Let x be the number of iterations of the j (upper) loop.
let j = i for some i
i/2^x < 1 <= i/2^(x-1)

Take log (in this course log means log base 2): 
log i - xlog2 < 0 <= log i - (x-1)log2
log i - x < 0 <= log i -x +1
log i < x <= log i + 1

The total cost at most:
Theta(1) + /sum_{i=1}^{n} (Theta(1) + Theta(1)(log i + 1))
= Theta(1) + Theta(n) + Theta(1)log(n!)
			        ^^^^^^^^
				Theta(nlogn)
= O(nlogn)


Omega-bound:
floor>= j/4, for all j >=2
Let y be the number of iterations of the j(lower) loop, j=i for some i
i/4^y < 1 <= i/4^(y-1)
log i - 2y < 0 <= log i -2y + 2
(log i)/2 < y <= (log i + 2) /2 

The total cost at least:
Theta(1) + Theta(n) + /sum_{i=1}^{n} (log i)/2
= Theta(1) + Theta(n) + Theta(nlogn)
= Omega(nlogn)


Therefore, Theta(nlogn).
___________________
Design of MergeSort

MergeSort(A,n)
************************************
1. if n = 1 then 			Theta(1)
2.	S <- A				Theta(1)
3. else
4.	nL <- ceiling(n/2)		Theta(1)
5.	nR <- floor(n/2)		Theta(1)
6.	AL <- [A[1],A[2],...,A[nL]]	Theta(n)
7.	AR <- [A[nL+1],...,A[n]]	Theta(n)
8.	SL <- MergeSort(AL,nL)		
9.	SR <- MergeSort(AR,nR)
10.	S <- Merge(SL,nL,SR,nR)
11. return S				Theta(1)
************************************

Let the T(n) denote the running time of the MergeSort on input of size n.

8.	SL <- MergeSort(AL,nL)		T(ceiling(n/2))
9.	SR <- MergeSort(AR,nR)		T(floor(n/2))
10.	S <- Merge(SL,nL,SR,nR)		Theta(n)

T(n) = { Theta(1), if n=1
       { T(ceiling(n/2)) + T(floor(n/2)) + Theta(n)

Then we need to solve the recurrence.
T(n) = { 2T(n/2) + cn, for some const c, if n>1 
       { d, for some const d,if n=1

Method I)
Solving by using Substitution method (Guess and check):
T(n) = 2T(n/2)+cn = 2(2T(n/4)+cn/2) + cn 
= 4T(n/4)+2cn = 8T(n/8)+3cn = ... = 2^i T(n/2^i) + icn
= nT(n/n) + cn log n = dn +cnlogn
Prove by Strong Induction: T(n) = nd + cnlogn 
Base case: T(1) = d
Inductive Hypothesis: Assume T(k) = kd+ cklogk, for all k<n
Inductive Step: T(n) = 2T(n/2) + cn = 2(nd/2 + cn/2 * log(n/2)) +cn
		= nd + cn(logn-1)+cn = nd + cnlogn

Method II)
Recursion-tree method:
each node represents a subproblem the value at each node represents none-recursive cost at each sub problem
					T(n)				cn
				       /    \
				    T(n/2)  T(n/2)			cn
				     /   \   /   \		
			     T(n/4) T(n/4) T(n/4) T(n/4) 		cn
                                   ..................			cn
			   T(n/n) T(n/n) T(n/n) T(n/n) T(n/n) T(n/n)    nd
		             d       d    d          d   d     d    
The height of the tree is logn
T(n) = nd + cnlogn

	


************************************
004-Jan16
Module2: Priority Queues
ADT(abstract data type): A description of information and a collection of operations on that info.
					  ```````````                     ``````````
				     	  data structure  		  algorithms
Note: the info in access ONLY through these operations.
			 ````		
______________
Dynamic Arrays		
Linked List: O(1) Insertion/deletion	O(n) to elem access
Arrays: O(1) elem access	O(n) Insertion/deletion

Two realization of Dynamic Arrays:
1) allocate a HUGE array
2) do Amortized analysis(CS341/CS466): allocate a small array initially, and doule its size as needed
_________
Stack ADT (LIFO)
Operations:
push(x)
pop()
isEmpty()
peek()

Realization of Stack ADT:
1. using arrays
Implementation:
Attribes:
top
size
length

[3|7|1| | | | | | ] -> length
     ^
    top size

push(x):
	A[size]	= x
	size = size + 1
pop():
	size=size-1
	return A[size]
So, push is O(1), pop is O(1)

2. using linked lists 
......


________________
Queue ADT (FIFO)
[ | | | | | | | | | | | | | ] -> length
       ^		 ^
       first		last


Operations:
enqueue(x):
	last = (last +1 ) mod A.length
	A[last] = x
	size = size + 1


______________
Priority Queue (like to-do list, simulation systems)
Priority Queue: An ADT consisting of collection of items(each has a priority) with operations
insert: inserting an item tagged with a priority
deleteMax: removing the item of highest priority

Note: the above defn is maximum-oriented. We can have another minimus-oriented priority queue. The operation is deleteMin.
Operations:
insert(key,item)
deleteMax()
isEmpty()
getMax()
update(item)
join(pq1,pq2) 

Sorting:
A = [7,1,5,3,2]
// insert all elem from A to PQ
PQ.insert(7,7)
PQ.insert(1,1)
....
PQ.insert(2,2)

// Extract all elem from PQ back to A
A[0] = PQ.extractmin()
....
A[5] = PQ.extractmin()


Implementation Options:

_Attempt1_______
unsorted array:
[7 1 3 4 11 0 2 17 ]
insert: O(1) 		just add elem at back
extractMax:O(n)	-search the array
		-swap the A[i] to A[n-1]
		-return A[n-1]
		-n=n-1  	
Using unsorted linked lists is identical
					______________
This realization used for sorting yield selection sort

_Attempt2_____
Sorted array:
[1 7 17 100 256 ]
now insert 10 into it:
[1 7 10 17 100 256 ...]
extractMax:O(1)
insert: O(n)
using sorted linked lists is identical
					______________
This realization used for sorting yield insertion sort

Can we do better than O(n) searches?
_____________________________________________________________________________
We don't scan the entire data structure, we want to skip large chunks -> Tree
Consider PQ: 2 2 4 1 5 7 6 8 3
The idea is: using the tree where the root has always the largest key.
				 8
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / \
			  1   2

Note: this tree is complete:
- all levels except the last one are complete full
- last level is filled from left to right     ____
A partially ordered complete tree is called a heap.

				 8
			       /   \
			      6     7
			     / \   / \
			    5   4 2   1
			   / \
			  3   2
		This is another way to store value.
		note that the parent always has high priority.
______
Heaps
A max-heap is a binary tree with the following two properties:
* Structural property: all levels except are complete filed except (possibly) for the last level. it item in the last level is left-justified
* Heap-order property: For any node i, key (priority) of parent of i is larger than or equal to key of i.
A min-heap is the same, but with opposite order property

				.
			       / \
			      .   .
			     / \ / \
			    .
			   / \
			  .   .

Lemma: Height of a heap with n nodes is Theta(log n)
__________________						 ____
height of the node  = # of edges in the longest simple path to a leaf
_________________						 ____
depth of the node = # of edges in the longest simple path to the root

n = 2^h -1 + k where 1<= k <= 2^h
=> h <= log n < h+1
=> h = floor(log n)
=> Height of the tree is floor(log(# of nodes in that tree)).

Impementation:
Store items level-by-level, top-to-bottom from left to right in an array
				 8
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / \
			  1   2

				0:8
			       /   \
			     1:7    2:6
			     / \   / \
			  3:5 4:3 5:2 6:4
			   / \
			 7:1 8:2
		(key:item)
                 => [8 7 6 5 3 2 4 1 2]
For element i(key):
left(i) = 2i + 1
right(i) = 2i + 2
parent of (i) = floor((i-1)/2), i != 0.

eg. 
i =2
left(i) = 5
right(i) = 6
parent(i) = 0
so, the A[2] left child is A[5].
______
Insert
eg. heap insert 9 to:
				 8
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / \
			  1   2

				 8
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / \  /
			  1   2 9

				 8
			       /   \
			      7     6
			     / \   / \
			    5   9 2   4
			   / \  /
			  1   2 3

				 8
			       /   \
			      9     6
			     / \   / \
			    5   7 2   4
			   / \  /
			  1   2 3

				 9
			       /   \
			      8     6
			     / \   / \
			    5   7 2   4
			   / \  /
			  1   2 3
Note after insert 9 to every end. 9 goes up and up!
		       ___________
This process is called "bubble-up".

Psudo-code:
heap Insert(A,x)  		[9 8 6 5 7 2 4 1 2 3]
**************************************
 A[size(A)] = x
1. index = size(A)
2. size(A) = size(A) + 1
3. while(index>0 and parent(index)<A[index])
4.	swap A[index] and A[parent(i)]
5.	index = parent(index)
**************************************





.
************************************
005-Jan21
__________________
Insertion in Heaps

				 8
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / \
			  1   2
                 => [8 7 6 5 3 2 4 1 2]

heapGetMax: Theta(1) 
heapdelMax:
To make sure the tree remains complete, we place the root with the rightmost element at the last level.


				 2
			       /   \
			      7     6
			     / \   / \
			    5   3 2   4
			   / 
			  1  

now we proform bubble-down (swap the current node 2 with the larger child 7):
				 7
			       /   \
			      2     6
			     / \   / \
			    5   3 2   4
			   / 
			  1

and again:  
				 7
			       /   \
			      5     6
			     / \   / \
			    2   3 2   4
			   / 
			  1  

Since, O(height of heap) = O(log n).
heapdelMax: O(log n).

psudo-code:
************************************
heapDeleteMax(A):
1. max = A[0]
2. heapSize(A) = heapSize(A) - 1
3. swap A[0],A[heapSize(A)]
4. bubble-down(A,0)
5. return max
************************************

********************************************************************
bubble-down(A,i):	// i is the element to be pushed down
1. while left(i) < heapSize(A) do
2. 	max = i, l = left(i), r = right(i)
3.	if A[l] > A [max] then max = l
4.	if r < heapsize(A) and A[r] > A[max] then max = r
5.	if max == i then break
6.	swap A[max], A[i]
7.	i = max
8. done
************************************************************************
Each loop iteration goes 1 step down the tree, since the height of heap is floor(log n), then bubble-down 's worst case running time is O(log n).


For a heap-based PQ(priority queue):
heapGetMax: Theta(1)
heapDeleteMax: O(log n)
heapInsert: O(log n)

________
Sorting
Idea: Turn A into a heap, then worst case can find the max in Theta(1), fixing the heap only takes O(log n) time.

Building a heap:
_____________________________________
Approach 1: Top-down creationg of map
eg. [2 4 6 8 1 3 5 7 ...]
Treat the first elem as a heap, and insert the rest of array into the heap.
[2| 4 6 8 1 3 5 7 ...]
 ^  ^^^^^^^^^^^^^^^^^^
heap   rest of Array

By proforming bubble-up one step we get:
[4 2| 6 8 1 3 5 7 ....]
Again:
[6 2 4| 8 1 3 5 7 ....]
[8 6 4 2| 1 3 5 7 ....]
........

psudo-code:
*************************
Top-Down-Heapify(A):
1. for i = 1 to n-1 do
2.	bubble-up(A,i)
*************************

Runtime of Top-Down Heapify:
Note:
we have n - 1 iterations
,and the cost of bubble-up is O(log n)
Then total cost is O(nlogn)

worst case: initial order is ascending.
Leaves have depth h or h-1 in worst case, # of swaps for bubble-up at least h-1.
we will have to perform at least: ceiling(n/2)(floor(log n) - 1)
______________
Complete tree
For any complete tree of size n, ceiling(n/2) nodes are leaves.


___________
Approach 2: what is we view the array as already a heap out of order, then we perform "fixing" it.
				 2
			       /   \
			      4     6
			     / \   / \
			    8   1 3   5
			   / 
			  7  
		    => [2 4 6 8 1 3 5 7 ...]
Look at 8: nothing to do
				 2
			       /   \
			      4     6
			     / \   / \
			    8   1 3   5
			   / 
			  7 
Look at 6: nothing to do
				 2
			       /   \
			      4     6
			     / \   / \
			    8   1 3   5
			   / 
			  7 
Look at 4: swap with 8, swap with 7
				 2
			       /   \
			      8     6
			     / \   / \
			    4   1 3   5
			   / 
			  7 
				 2
			       /   \
			      8     6
			     / \   / \
			    7   1 3   5
			   / 
			  4 
Look at 2: swap with 8, swap with 7, swap with 4
				 8
			       /   \
			      2     6
			     / \   / \
			    7   1 3   5
			   / 
			  4 
				 8
			       /   \
			      7     6
			     / \   / \
			    2   1 3   5
			   / 
			  4 
				 8
			       /   \
			      7     6
			     / \   / \
			    4   1 3   5
			   / 
			  2 

Note: we do not have to look at leves. only look at internal nodes:
[           |            ]
             ^^^^^^^^^^^^
              leaves
just look at all element < floor(n/2)

pesudo-code:
*********************************
Bottom-up Heapify(A):
1. for i = floor(n/2) - 1 down to 0 do
2. bubble-down(A,i)
*********************************

Runtime Analysis:
# of nodes	height of nodes
2^0		h
2^1		<= h -1
2^2		<= h -2
...
2^h		<= 0
There are no more than 2^(h-i) nodes if height i.

Total # of swaps is at most, ie:
<= \sum_{i=1}^{h} i*2^(h-i) = 2^h \sum_{i=1}^{h} i/2^i <= n \sum_{i=1}^{h} i/2^i < n \sum_{i=0}^{infity} i/2^i = 2n
													         ```
Now we need to solve the sum:
let s = 1/2 + 2/4 + 3/8 + ....
   2s = 1 + 1 + 3/4 + 4/8 ....

(2s -s ) = 1+ 1/2 + 1/8 + 1/8 + ....
	 = 2

 we can convert an array to a heap in Theta(n) time.

***************************************************
HeapSort(A,n)
1. Heapify(A)	//using bottom-up approach
2. for i = 0 to n-1 do
3.	A[n-1-i] = heapDeleteMax(A,n-1)
****************************************************
************************************
006-Jan23
_________________
Selection Problem
Given an array A[0..n-1] and 1 <= k <= n, return the k-th largest element in A.
1. sort A in descending order, return A[k-1]
cost:  Theta(nlogn) for Heap sort
2. scan A, k times, delete max each time
cost:  Theta(kn)

eg.
A = [6 5 3 8 7 4]
Find the three largest elem
6| 6,5| 6,5,3| 6,5,8| 6,7,8| 6,7,8 => 6

3.
Find k-th largest element
we can just use min-heap to implement that priority queue.
build heap: Theta(n)
look-up and replace element O(log k)

Total cost = O(nlogk)
Note: it is good if k is small, if k = 1 (find max) , it is O(n)

4. Heapify(A), call deleteMax(A) k times 
cost + n + klogn
if k ~ n/2 (around n/2) ,same as sorting
if k in O(n/logn) then O(n)
_________________
Can we do better?

Selection problem: (restated differently)
Give an array A[0..n-1] and 0<=k<=n-1, find the element at position k of the sorted array A.

Yes!  Selection problem can be solved in linear time* with QuickSort.

eg. Let A = [7 3 2 4 6 1]
	     0 1 2 3 4 5
sorted(A) = [1 2 3 4 6 7]
what is the position of A[3] in the sorted(A)? (assume all elem in A are distinct)
soln: the position of A[3] is # of keys <A[3] in A[0..2] and A[4..5]
=> 3
that is sorted(A)[3] = A[3]
The runtime is linear.

Idea: choose an element(pivot) and partition the data into (item < pivot), pivot, (item > pivot)
if position(pivot) = k, done!
otherwise we countinue either on the left or on the right depending on the position of the pivot.

How should we pick the pivot?
(usually want to split in half (recall BinarySearch))

let's just pick the first element.
choose-pivot1(A):
   return 0

Partitioning Algorithm:
Input A = [5| 4 9 8 6 3 2]
	      ^         ^
	  pos i     pos j
Advance i, backup j:
A= [5| 4 9 8 6 3 2]
         i       j
since A[i] > A[j], swap i and j
A= [5| 4 2 8 6 3 9]
Advance i, backup j:
A= [5| 4 2 8 6 3 9]
	   i   j
swap ij and advance i , backup j
A= [5| 4 2 3 6 8 9]
     	   j i  
now j < i, we stop
and swap pivot with j
A= [ 3 4 2 5 6 8 9]	

**************************************
partition(A,p)
A: array of size n, p: int 0<= p < n
1. swap(A[0],A[p])
2. i = 1

...
**************************************

if any other element of the array is chosen to be pivot, just swap with A[0].

choose-pivot1: Theta(1)
partition 1: Theta(n), Theta(1) extra space

********************************************
QuickSelect1(A,k)
 p = choose-pivot1(A)
 i = partion(A,p)
 if i = k then
	return A[i]
 if i > k then
	return QuickSelect1(A[0..i-1],k)
 if i < k then
	return QuickSelect1(A[i+1,n-1],k-i-1)
*********************************************

cost of QuickSelect
T(n) = Theta(n) + { Theta(1) ,if i = k
	^	  { T(i)     ,if i > k
     partition	  { T(n-i-1) ,if i < k

----------------------------------------
best case:
T(n) = Theta(n) , it happens when i=k, which is the first chosen pivot is the element at position k, no recursive calls

worst case:
if i = 0, or i = n-1, it happens when the array is sorted in ascending or descending order.
T(n) = 	{ d,		if n=1
	{ T(n-1) + cn	if n>=2
	    ^ 	    ^
	recursion   partition
	on left
	or right
  
T(n) = cn + c(n-1) + c(n-2) .... + c*2 + d
     = c(n+1)n/2 - c + d in Theta(n^2)
_____________________________________
What if the partitioning is balenced?
A[p] will be around median:
T(n) = { d		,if n=1
       { T(n/2)+cn	,if n>=2
Assume n is a power of 2, n = 2^i
T(2^i) = c*2^i + c*2^(i-1) + ... +c*2^1 + d
       = c(2^i + 2^(i-1) + ... + 2) = c(2^(i+1)-2^0) + d 
       = 2c(n-1) + d  in Theta(n) which is the same as best case
______________________
Average case analysis:
average cost of all inputs of size n, as a function of n.
Assumption: distribution is uniform, all kind of inputs are equally likely
A_1 = [1 3 5 7]
A_2 = [4 5 6 7]
Observation:
behavior depends relative ordering, not on acutal values of keys

There are n! possible ordering of keys x1...xn, and each ordering is equally likely
L #of items	R #of items
0		n-1		x_1 wiil be A[0] for (n-1)! orderings
...		...
n-1		0

T(n,k) = cn + 1/n T(n-1,k-1)
	    + 1/n T(n-2,k-2)
	    + ...
	    + 1/n T(k+1,k)




 

************************************
007-Jan28
QuickSelect1: Average Case Analysis:
Observation: the behaviour depends on relative ordering, not on actual values of keys:
[1 2 3 4] and
[2 5 8 11] will yield the same worst case behaviour.

Let's assume keys are distince, and keys are x1,x2,....,xn
Then, there are n! possible orderings of keys, and each ordering is equally likely.

L#ofitems	R#ofitems
0		n-1
1		n-2
...			doing recursion on the right
k-1

---
k			-> this means the pivot is the answer
---

k+1
...			doing recursion on the left
n-1		0


Define T(n,k) as average cost for selecting k-th element from size-n array.
T(n,k) = cn + (n-1)!/n! T(n-1,k-1) + 1/n T(n-2,k-2) + ... + 1/n T(n-k,1) + 1/n 0 + 1/n T(n-1,k) + 1/n + ...
    + 1/n T(k+1,k) 
T(n,k) = cn + 1/n (\sum_{i=0}^{k} T(n-i-1,k-i-1)+\sum_{i=k+1}^{n-1} T(i,k))

Define T(n) = max_{0<=k<=n-1} T(n,k)
We will not acctually calculate the cost, instead we will find the upper bound of the cost

Observation: 
A = [****| .... X .... |*****]
	n/4  	      3n/4
At least 1/2 of all n! ordering will have the pivot at n/4 <= i < 3n/4 that is i in ceiling(n/4) ...... floor(3n/4).
T(n) <= cn + 1/2 T(3n/4) + 1/2 T(n)

T(n) <= 2cn \sum_{i=0}^{infty} (3/4)^i + d = 8cn + d  in O(n)
T(n) in Theta(n) on average case.
___________________________________
Speace requirement for QuickSelect:
Pivoting/Partitioning: Theta(1) constant additional space
Recusive calls: allocates space on the runtime stack
But recursive call is the last thing the function does, so it is a tail recursionâ€Ž.
Note: A good compiler will turn a tail-recursion call into a loop, so hopefully no stack usage
(compiler will turn tail recursion into a for loop, thus no stack usage.)


What is bad about the QuickSelect1?
Slow on some inputs (sorted,or reverse-sorted list)
What can we do about this?

First idea: Randomly permute the input first using shuffle.
Second idea: change the pivot selection.

_________________________________________________
Difference between average and expected runtime?
Average: any one input can be good or bad, but the average of all in Theta(n).
Expected: every input is probabilistically O(n), but with unlucky random # generation, any
	input can be slower bad.
Idea: generate good pivot, deterministically
Median of medians: Assume all keys are distinct. 
Select(A,k)
1. Divide the array into blocks of size 5
x = floor(n/5) blocks
2. Find the median in each block. (1,2 Take Theta(n))
3. Recusively select the median of medians of these blocks. T(floor(n/5))
4. Partition A with the median and medians as a pivot
5. Proceed in the same way as we did in QuickSelect1
-----------------------below is beyond the scope of this course---------------------
T(n) approx= cn + T(n/5) + T(7n/10) in Theta(n) (The constant for n is pretty big! In real we just randomly choose pivot.)


_________
QuickSort
***********************
QuickSort(A[0..n-1])
	if n<=1 return
	p = choose-pivot(A)
	i = partition(A,p)
	QuickSort(A[0..n-1])
	QuickSort(A[i+1..n-1])
***********************

Note: at least one of the tails is not tail-recursive. We want always make sure that the tail call is done on the larger of two sub-problems.
The none-tail recursive call will be on input of size <= n/2. The recursion depth on the non-tail call is O(log n).

Worst case: pivot at the end or at the front 
T(n) = cn + T(n-1) in Theta(n^2)
Best case:  balanced-case.
T(n) = T(floor((n-1)/2)) + T(ceiling((n-1)/2)) + cn
	 = 2T((n-1)/2) + cn < 2T(n/2) + cn in O(nlogn) 
Balanced is not usually possible: what if partition splits n/10 and 9n/10
				    cn
			      /    \
			cn/10        9cn/10
		   /  \          /  \
     cn/100 9cn/100 9nc/100 81cn/100
			...
	Left branch goes pretty fast than the right branch.












************************************
008-Jan30
__________________________
Average case for QuickSort
After partition
A = [***... X ...***]

T(n) = cn + 2/n (T(0) + T(1) + ... T(n-1))

nT(n) = cn^2 + 2 (T(0) + ... + T(n-1))

(n-1) T(n-1) = c(n-1)^2 + 2(T(0)+...+T(n-2))

nT(n) - (n-1)T(n-1) = 2cn -c + 2T(n-1)

nT(n) = (n+1)T(n-1) + 2cn - c

T(n)/(n+1) = T(n-1)/n + (2cn -c)/(n(n+1)) <= T(n-1)/n + 2cn/(n(n+1)) - T(n-1)/n + 2c/(n+1)

T(n-1)/n <= T(n-2)/(n-1) + 2c/n

T(n-2)/n <= T(n-3)/(n-2) + 2c/(n-1)

....

T(n)/(n+1) <= T(1)/2 + 2c/3 + 2c/4 + .... + 2c/(n+1)
         ^
         d
     = d/2 + 2c \sum_{i=3}^{n+1} 1/i
     = d/2 + 2c(H_{n+1} - H_3) in O(log n) 
H is harmonic sequence.

Since T(n)/(n+1) in O(log n).
Then T(n) in O(nlogn) 


____________________
Randomized QuickSort
Randomly pick a pivot.
E(T(n)) = cn = T(k) + \sum_{k=0}^{n+1} T(k)+ T(n-k-1)*P(i=k)
	= cn + 1/n (\sum_{k=0}^{n-1} (T(k)+T(n-k-1)))


________________________
Lower Bounds for sorting
Question: can we do better than nlogn
yes: non-comparison can be n
no: best speed for comparasion sort is nlogn

Theorem: Any correct comparasion-based sorting algorithm must have a worst-case of at least Omega(nlogn).
To see this, we introduce decision trees.
That we use to model execution of an algorithm on all inputs.
-> sequence of question
-> bracnch on the answers
-> leaves are the outcomes

For sorting algorithms, questions are of the form A[i]<A[j], and answers are yes/no.
Each question tells you something about the order of the elements in the array by the time you reach the bottom of the tree.

Example:
A = [a b c]
		   A[0] < A[1]?
	    Yes /               \ No
	  A[0]<A[2]?       A[0] < A[2]?
      Yes/        \NO	 Yes/         \No
    A[1]<A[2]?  [c a b] [b a c]     A[1] < A[2]?
 Yes/       \No			   Yes/       \No
[a,b,c]    [a,c,b]		  [c a b]    [c b a]

The worst case is the height of the tree.
The average case is the average length of paths from root to a leaf.
An array of size n can be permutated in n! different ways.
=> the decision tree must have at least n! leaves.
2^h >= n!, h>= log(n!) = n/2 (logn -1) in Omega(nlogn)
So, at least Omega(nlogn) comparasion must be done in the worst case.
-> worst case running time for a comparasion-based sorting algorithm must be at least Omega(nlogn).
________________________________________________
Can we sort faster than Theta(nlogn) worst case?
CountingSort(A[0...n-1])
- the item in A are all in range 0..k-1
************************************************************
 Array C[0..k-1], filled with zeros
 for i = 0 to n-1 do								Theta(n)	
	c[A[i]] = C[A[i]] + 1 		//c[i]=|{key=i}|
 for i = 1 to k-1 do								Theta(k)
	c[i] = c[i-1] + c[i]		//c[i]=|{key<=i}|
 B is a copy of A								Theta(n)
 for i = n-1 downto 0 do							Theta(n)
 	C[B[i]] = C[B[i]] -1
	A[C[B[i]]] = B[i]
**************************************************************
Total time: Theta(n+k)
Total Space: Theta(n+k)

Input: A= [4 5 7 0 5 5 10], k =11
           0 1 2 3 4 5 6
c = [0 0 0 0 0 0 0 0 0 0]
     0 1 2           9 10
After the first loop:
c = [1 0 0 0 1 3 0 1 0 0 1]
     0 1 2 3 4 5 6 7 8 9 10
After the second loop
c = [1 1 1 1 2 5 5 6 6 6 7]
     0 1 2 3 4 5 6 7 8 9 10

Entry 		In sorted Array
0		0..0
4		1..1
5		2..4
7		5..5
10		6..6

The element with key i must be stored at the indices c[i-1] .. c[i]-1
B = [4 5 7 0 5 5 10]
After i = 6, c = [1 1 1 2 5 5 6 6 6 6 ]
A = [_ _ _ _ _ 10]
		6
c = [1 1 1 1 2 4 5 6 6 6 6]
After i = 5,
....
A = [0 4 5 5 5 1 10]


Does is always faster than comparasion sort?
No! A = [1 2 10000000]
The time will be bounnded by 10000000
if k>>n, it does not worth using counting sort.

CountingSort is stable:
A = [(Jam,10),(Tom,17),(Bob,10)]
unstable Sorted: [(Bob,10),(Jam,10),(Tom,17)]
stable sorted: [(Jam,10),(Bob,10),(Tom,17)]

Equal items stay in the same order in the sorted array, if the algorithm is stable.

Idea: split the keys into fragments and perform counting sort according to each fragment.

Example:
1 3 4      3 7 0	1 3 4
4 8 4	   4 6 2	1 3 5
2 4 5   => 1 3 4  =>  	2 4 5
1 3 5	   4 8 4	4 6 2
3 7 0      2 4 5	4 6 4
4 6 5	   1 3 5	4 7 0
4 6 2	   4 6 5	4 8 4
    ^        ^	        ^
  sort	    sort	sort
************************************
009-Feb4
__________
Radix Sort
We consider keys as d-digit base k numbers (0 to k-1), each key has a unique representation:
Xd-1 Xd-2 .... X0
eg. for key 237, x0=7,x1=3,x2=2, d=3, k=10
     11101101, d=8,k=2    x0,...,x7=1,....,1

radix-sort(A,d,k){
    for i from 0 to d-1 do		// d time
	countingsort based on digit i  	// O(n+k)
}

Total runtime: O(d(n+k)), or O(dn)


_______________
ADT Dictionary
Facilitates key-value look-up
- contains at most one occurrance of a given key
- and for each key, exactly one value

Operations:
insert(k,v) - add (k,v) to dictionary
delete(k)   - remove the pair with key k
search(k)   - returns v such that (k,v) is in the dictionary


Implementation Options:
1) Array/Linked list (unsorted) 	-> Linear time search/delete  ->constant time insert
2) sorted Array				-> Linear time insert/delete  ->Logarithmic search
 
__________________
Binary Search Tree
search(k) Compare k to current node, stop if found else recurse on subtree unless it's empty
delete(k): if node is a leaf, just delete it
	   if node has one child, move child up
	   else, swap with successor (move right then left left left ....) or predecessor (move left and right right right ...) node and then delete

Operations run in Theta(h) time where h is the height of the tree
it is: O(log n) if the tree is optimal height
       O(n) if the tree is unbalanced 
	
	1
	 \
	  2
	   \
	    3
	     \
	      5
__________
Definition
A binary tree is called balanced if it is empty or if its children have height differ by at most 1, and are balanced 
height of an empty tree is -1.
Theorem: A balanced tree has height at most 1.44 log n.
Proof: Turn the question around and ask: what is the least number of nodes in a balanced binary tree of heiht h

h=-1 , n=0
h=0 , n = 1
		.
h=1 , n = 2
		.
		 \
		  .
h=2, n = 4 

		.
	       / \
	      .	  .
	     /
            .
h=3 ,
		 .
	       /   \
	      .	    .
	     / \   /
            .   . .
      	   /
          .

Define T(h) to be a min # of nodes in a worst-case tree of height h.

A worst case is when the right subtree has height 1 less than left for each node
T(h) = 1 + T(h-1) + T(h-2)
1 + T(h) = 1 + T(h-1) + 1 + T(h-2)

{1+ T(h)} is Fibonacci Sequence
let G(h) = 1 + T(h)

G(0) = 2 = F3
G(1) = 3 = F4
...
G(h) = F_{h+3}

T(h) = F_{h+3} - 1
    approx = a^(h+3) / sqrt(5) - 1 where a is (1+sqrt(5))/2
n <= ... = log h / log 4 = 1.44 log n


So, we don't need a perfect tree - if we can maintain the tolance we'll get logarithmic height
=> logarithmic scarhes

How do we maintain torlance?
Solution: (due to Adelson-Velskii and Landis ,1962)
_________
AVL tree
Definition: For each node N in a binary tree, define balance(N) = height(Right(N)) - height(left(N))
Store a balance field in each node to keep track of its balance.
In a binary tree that is balanced, each nodes balance is -1,0,1

Operation for AVL trees:
search: identical to BST search  (BST runtime: can be O(log n) or O(n) depend on O(h))
	but AVL search is garentee to be O(log n)
Insert: similar to BST insert. BUT inserting a new node can violate the balance property









************************************
010-Feb6
Definition:					 _____________
When an item is inserted into an AVL tree T, the critical node is the last node on the path from the root to k with a non-zero balance (before k was inserted)

eg.
	 .
       /   \
      .     c
     /_\   / \ 
     	  .     .
         / \   / \
        .   . .   .
             / \  /
            .   ..
(recall: a tree is balanced if balance on each node is one of -1(left heavy),0,1(right heavy))

Note: there may not be a critical node in the tree
Fact: the only nodes whose balances change upon insertion are nodes on the path from the critical node to the new node. (If no critical node)
-> called crtitical path

_____________
Left Rotation
1
 \
  2
   \
    3

=>
  2
 / \
1   3




______________
Right Rotation
    1
   /
  2
 /
3

=>

  2
 / \
1   3



_______________
Double rotation

1
 \
  3
   \
    2

=> 
 1
  \
   2
    \
     3
=>
  3
 /
1
 \
  2


__________
Insertion

_____
Case1:
-	A(1 -> +2 after insertion)
|     /	  \
     .	   B (0 -> 1 after insertion)
 h-2/E\   / \
h        .   .
     h-2/C\ /D\ h-2
|            ^
_	sth goes here


Case1:
After the left rotation:
	  B (0)
      /      \
      A(0)   /D\ h-2
    /   \     ^
h-2/E\  /C\ insert item

______
Case2:
-	A(1 -> +2 after insertion)
|     /	  \
0->1 B     E
    / \
h  .   .
  /C\ /D\ h-2
|  ^
sth goes here

Case2 after right rotation:
	B
      /   \
     /C\   A
      ^   / \
    item D   E 



_______
Case 3:

-	A(1 -> +2 after insertion)
|     /	  \
     .	   B (0 -> -1 after insertion)
 h-2/E\   / \
h        .   .
     h-2/C\ /D\ h-2
|        ^
_sth goes here

Just perform Double Rotataion: 
1) right rotation through B
2) rotation to the left through A

Case3 after double-rotation:
        A
      /	  \
     E     B
          /  \
          C1  D
         / \
        C2  C3
        ^
       item

1)(rotate C2 C1 B) =>
	A
      /   \
     E     C1
         /   \
        C2    B
        ^    / \
      item  C3  D

2) (rotate A C1 b)=>
  	C1
      /    \
     A      B
    / \    / \
   E   C2 C3  D

_______
Case 4:
-	A(1 -> +2 after insertion)
|     /	  \
0->1 B     E
    / \
h  .   .
  /C\ /D\ h-2
|      ^
  sth goes here

Just perform Double Rotation:
1) rotation to the left through B
2) rotation to the right through A

case 4 after double rotation:
	A
      /   \
     B     E
    / \
   D   C1
      /  \
     C2  C3
     ^
    item

1) (rotate C3 C1 B)=>
	A
      /   \
     C1    E
    /  \
   B    C3
  / \
 D   C2
      ^
     item

2) (rotate B C1 A)=>
	C1
      /    \
     B      A
    / \    / \
   D  C2  C3  E
      ^
     item


Because the height of the critical node stays the same after the rotations, no further rotation will be required.
=> Balance is now restored throughout the entire tree.
=> After the insertion, at most one (single/double) rotation is needed to restore balance, and if needed it will happen at the critical node.

=> runtime of insert if O(log n)


________
Deletion
Use the standard BST delete
But: this can destroy the balance of the tree. If a node is deleted, its parent's balance changes

_____
case1
Parent's balance goes from 0 -> +- 1
=> Height of the tree remains the same, so other balances do not change, done

	. (0->-1)
       / \ 
      x   .

	. (0-> 1)
       / \ 
      .   x

_____
case2
Parent's balance goes from +- 1 to 0
=> the height of the parent's subtree changes 
=> must rejust grand parent's balance

	. (-1 -> 0)
      /   \
-1->0.     .
    /
   x

	. (1->0)
      /   \
     .     . (1->0)
            \
             x

_____
case3
Parent's balance goes from +- 1 to +- 2
=> must rebalance the parent
=> In the worst case, need to rebalance parents all the way to the root O(log n)

	E
      /   \
1->2 B     F
    / \     \
   x   C     G
        \
         D
Height of the subtree changes, so need to rebalance the grand parent
performe left rotation on (B C D):
	E (0)
      /   \
  (0)C     F
    / \     \
   B   D     G








 

************************************
011-Feb11
_________
2-3 Trees
An alternative to AVL trees:
multi-way search tree: 2-3 trees
AVL trees: keep leaves at "mostly" the same level.
2-3 trees: keep leaves at exactly the same level by varying the sizes of the nodes

Leaves: all at the bottom level contain 1 or 2 keys.
Internal nodes: either contains 1 key + 2 children, (called 1-node)
		or 2 keys + 3 children,	(called 2-node)
1-node eg:
	k
      /   \
     L     R

All keys in L are smaller than k.
All keys in R are bigger than k.

2-node eg:
	k1,k2
      /   |   \
     L    M    R
All keys in L is smaller than k1.
All keys in M is > than k1 ,and < than k2
All keys in R is bigger than k2.


2-3 tree example:
			     14,24
		     /        |          \
	            4,8       18         30
		  /  |  \     /\       /    \
 		 2 5,6 10,12 16 26,28 26,28  32
______
Search
same as for BST , except for 2-node

______
Insert
Use standard BST in sert, find the appropriate leaf, if the lead has one key, add the new key.
If the leaf has two keys already, add the new key, split the leaf into 2, promote the middle key to the parent.
eg .	
	18
       /  \
      16  20,22

insert(21)

	18
       /  \
      16   20,21,22

split the node into 2 part.
promote the middle key to the parent node: 21

	18,21
      /   |   \
     16   20   22 


If the parent has a 1-node, it will becomes a 2-node after insertion, done!
If the parent was a 2-node, it need to be split as well. 
->Middle key promoted to grandparent.

eg.


			     14,24
		     /        |          \
	            4,8       18         30
                  /  |  \     ....       ....
	         2  5,6 10,12 
Now insert(11).

			     14,24
		     /        |          \
	            4,8       18         30
                  /  |  \     ....       ....
	         2  5,6 10,11,12 


			     14,24
		     /        |          \
	            4,8,11    18         30
                  /  |  \ \   ....       ....
	         2 5,6 10  12 

			  8, 14,24
		     /    |   |          \
	            4    11    18         30
                  /  |  / \   ....       ....
	         2 5,6 10  12 


			     14
			  /        \
			 8         24
		     /    \     /         \
	            4    11    18         30
                  /  |  / \   ....       ....
	         2 5,6 10  12 


________
Deletion
If the key is in a leaf, remove the key from the leaf.
If not, swap the node with its in-order successor(that will be a leaf) and delete the key from the leaf

But: this could produce a leaf with no keys. 

eg. 

			        14
			  /           \
			 8             24
		     /    \         /          \
	            4     11      18,21         30
                  /  |   / \      / | \        /  \
	         2 5,6 10  12  16 20 22    26,28  x


			        14
			  /          \
			 8            24
		     /    \         /          \
	            4     11      18,21         28,30
                  /  |   / \      / | \        /  \
	         2 5,6 10  12  16 20 22      26    x

			        14
			  /           \
			 8              24
		     /    \        /          \
	            4     11      18,21         28
                  /  |   / \      / | \        /  \
	         2 5,6  10  12  16 20 22    26     30

Look at the node's immediate sibling, if the sibling has 2 keys, shift a key to the parent, parent to empty node.
If the sibling has one key, remove the node, push parent's key down to sibling.

Deletion cost: Theta(log n).

(First choice: borrow from siblings!)
 
				   14,24
			  /          |           \
			4,8	   18,21         28
 	              /  | \       / |  \   	/  \
		     2 5,6 10,11  16 20 22     26  30

delete 26

				   14,24
			  /          |           \
			4,8	   18,21         28
 	              /  | \       / |  \   	/  \
		     2 5,6 10,11  16 20 22     x   30




....


Finally,

				   14,21
			  /          |           \
			4,8	     18          24
 	              /  | \        / \   	/  \
		     2 5,6 10,11   16 20      22   28,30


Make sure that each node is either a 1-node (1 key 2 children), or 2-node (2 key 3 children).
And all tree mantain the level property.

____________________________________
Generalization (a,b)-trees / B-trees
What if we let nodes contain more keys?
- shorter trees
- but greater trees cost to procces each node.

when might this be a good idea?
If the dictionary is too large to fit into main memory, it need to store on disc, and it can hold only a small amount on RAM.
__________
(a,b)trees
internal nodes have >= a children 
		    <= b children
a>=2, b>= 2a -1. (the reason for b is when insertion occurs, we will be able to split the node into half)
root can have anywhere from 2 to b children.
All leaves are at the same depth.
______
B-tree
If b = 2a -1, we have a B-tree. We want optimal disk usage-> choose b as a large as possible but still fit in a disk block.

eg. 
If a = 100, b = 199.
A B-tree of min-size d has internal nodes with at least d keys and at most 2d keys.
min# of KVPs in a B-tree of height h of minisize d. 

The root is special it only has 2 children:
level0		  .	  1 node
	       /     \
level1       ....   ....   2d nodes
            //||\\  //||\\
level2	    .....		2(d+1)*d nodes
	   //||\\
level3	   .....		2(d+1)^2 *d
	  //||\\
level4	  ....			2(d+1)^3 *d

level h   ....			2(d+1)^(h-1) * d

Total min # of B-tree nodes is 1+ 2d \sum_{i=0}^{h-1}(d+1)^i = 2(d+1)^h - nodes

************************************
012-Feb13

---------------------------------------------------------------------------------------------------
That's the end of midterm
---------------------------------------------------------------------------------------------------

________________
Height of B-tree

Total: 1 + \sum_{i=0}^{h-1} 2d(d+1)^i = 2(d+1)^h - 1

so, if a =100, b= 199, d=99 
n = 10^6
ceiling(log(99,10^6)) = 4 disc access
For an AVL tree, log(2,10^6) approx = 20 disc access

Does it mean search B-tree is faster than AVL tree?
No.
It takes log d to search for each node.
Total cost O(log n / log d * log d) = O(log n) 
exactly the same as AVL tree.

The advantage of using B-tree is that it saves the time of Memory access (which is really expensive)
key(on the node)
		7   ,   13   ,   18
              /     |        |      \
           1,3,4,5  10,12     15,16   19

(Associated value):
4,2,4,5,6  <-> 8,11,13 <-> 14,16,17 <-> 18 20

The tree traversal is like this (using key):

1,3,4,5 -> 7 -> 10,12 -> 13 -> 15,16 -> 18 -> 19

______________________
Lower Bound for search
Theorem: In the comparison model, Omega(log n) are required to search in size-n dictionary.
Proof: any search algorithm can be modeled with a binary decision tree. Each leaf cooresponds to a possible outcome.
The key we are looking for could be any of n keys in the dictionary (ie. n possible outcomes = leaves)
or it could be not found (+ 1 outcome)

n + 1 (leaves) <= 2^h (the maximum node of binary tree) 
h >= log(n+1) in Omega(log n)
This matches the upper Bound O(log n) that comes from a balanced BST.

So, in order to be faster. We using the same idea for counting sort.

__________________________________________________
Non-comparison based implement of a Dictionary ADT

________________________________
Direct Access/ Direct Addressing
If you know the keys in 0,...,M-1, the best way to implement a dictionary is with a key-indexed array.
And, if M in Theta(n).

A = [0 |1    | .......  |M-1 ]
val  * |empty| 	        |empty

search(k); return A[k] 		Theta(1)
insert(k,v) A[k] = v		Theta(1)
delete(k); A[k] = empty		Theta(1)

Why don't we always use arrays?
Because
- keys are not integers
- when key are numbers than are much bigger than n.

_____
Goals
- reduce storage requirement
- still get Theta(1) runtime for all operations

____
Idea
what if there is a function h: key -> [0,..,m-1]
that is it maps call key into array indices
we can then store (k,v) in the dictionary as follows:
A[h(k)] = V

h -> called a hash function
A -> called a hash table
m -> called a table size
typically m is much smalller than |n|. Key k hashes into index/slot/bucket A[h(k)].

Example: Keys are Bear, Horse, Tapiz, Panda, Ox
                  ^     ^      ^      ^      ^
h: keys -> [0..25]
h(k) = position of k'th first letter in the alphabet.

A:
0
1 Bear
...
7 Horse
..
14 Ox
15 Panda
..
19 Tapiz
..
25

Now, what if we add the key Blobfish, we will have the overlap
h(Blobfish) = 1
h(Bear) = 1

If the position is already occupied, we have a collision.
If n <= m, a collision may or may not happen
If n > m , definity happens (Pigeon-hole principle, n slot, n+1 pigeon, there exists a slot which contains two pigeons)

____________________
A good hash function
- should be deterministic, a given input k should always produce the same h(k)
- similar keys should NOT be mapped to similar hashes (so that trends in the input do not yield trends in hashes)
- depend on all parts of the key
- must be cheap to compute
- should scatter the key in a way that looks random.
  Uniform Hashing Assumption
 if k1 = k2, Pr(h(k1)=h(k2)) = 1/m.

_____________________
Basic Hash Functions:
____________________
Hashing by division:
h(k) = k mod m, where m is the size of a table.

eg. keys 12,17,27,32,57
m = 10.
h(12) = 2
h(32) = 2
h(17) = 7
h(27) = 7
h(57) = 7
It is a bad hash. It just count the last digit.
			     ______		    _____________
(usually we choose m to be a prime number  which is not too close to 2's power or 10's power)

_________________________
Hashing by multiplication
Let {x} denote x - floor(x), fractional part of x, eg. {1.57} = 0.57
Let t be irrational.
Fact: the fraction {t}, {2t}, {3t} fill the interval 0<x<1 uniformly.
If we take t = 1/phi which = phi - 1 = (sqrt(5) - 1) /2 approx = 0.618

t	0.618	
2t	0.236
3t	0.854
4t	0.472
5t	0.90
7t	0.708
8t	0.454
9t	0.572
--------------

we see 6 2 8 4 9 7 4 2 

h(k) = floor(m*{k*t})















************************************
013-Feb25
___________________
Basic hash function

Divison method: h(k) = k mod M, where M is a prime not close to a power of 2

Back to collision resolution:
First strategy: Separate Chaining (Open hash)
Elements of the hash table are linked lists (or some other dictionary implementation, eg. AVL trees)


|-------|
|     --|------> [Bear| Null] 
|-------|
|	|
|-------|
|	|
|-------|
|     --|------> [Horse| Null]
|-------|
|	|
|-------|

Insert(Blobfish):

|-------|
|     --|------> [BlobFish| --]-> [Bear| Null] 
|-------|
|	|
|-------|
|	|
|-------|
|     --|------> [Horse| Null]
|-------|
|	|
|-------|

Search(k) search in the linked list at A[h(k)]

____
Cost
We will count probes, any memeory access, either:
* access an array element, or
* follow a link
Worst case: when all element are in the same bucket
1 probe into array + scan the entire list
1 + n in Theta(n)
-> indicates a really bad hash function
_____________
Expected case
Let n = # items in the table 
m = size of the table
alpha = n/m is called the load factor.

Note: alpha can be >1, ==1 , or <1
small alpha means wasted space
large alpha means cost of operation is likely to be expensive 
So we want n in Theta(n)

___________________
Unsuccessful search for some key k:
1 probe to access A[h(k)]
traverse the entire list at A[h(k)]
Expected length in the chain for n keys and m slot is: (Based on the Uniform assumption)
sum_{i=1}^{n} 1/m = n/m = alpha.
=> the cost of unsuccessful search is 1 + alpha in Theta(1+alpha)

_________________
Successful search  
Not counting the initial probe into the array:
For a chain of length 1, 1 probe finds the key.
For a chain of length 2, (1+2)/2 probes on average
For ...	       length 3, (1+2+3)/3 probes
For 		      k, 1/k sum_{i=1}^{k} i = (k+1)/2 probes
On average, k = alpha = n/m,	so 1 + (alpha+1)/2 probes.

But,
we never scan an empty list (for successful search A[h(k)] will not be empty)
=> so any list of a non-empty list is slightly larger than alpha

Suppose K1,K2,...,Kn are the n keys in the hash table, and they were inserted in that order.
Let Ci denote # of comparisons in a successful search for key Ki.
Ci =  1       +    1			+ | {i'>i: h(Ki)=h(Ki')}|
      ^            ^
array access   for comparison of Ki

   =  2 + (n-i)/m
   
Then the expected # of comparisons in a successful search is the average expect value of Ci,
as i ranges from 1 to n.
1/n sum_{i=1}^{n} Ci = 1/n sum_{i=1}^{n} (2 + (n-i)/m) = 2 + n(n-1)/(2mn) = 2 + (n-1)/m < 2 + alpha/2
in Theta(1+alpha)


So we have:
insert O(1) worst case
delete same cost as search

If n is in Theta(m) then the expected cost of all operation is Theta(1).

__________________________________
How do we pick m (hash table size)?
First we start with a reasonably small table, eg. 128
- grow/shrink when necessary (need to rehash all elements after that)


________________________________
Open Addressing (Closed Hashing)
- Store keys directly in the hash table
(n <= m, alpha <= 1)
- find the next available slot if collision occurs -> by using a porbe function
H(k,p) = the (p+1)th address you try for key k.
H(k,0) = h(k)
For key k, first try H(k,0) if that's full try H(k,1)
			    if that's full try H(k,2)
				....
H(k,0), H(k,1), H(k,2).... called a probe sequence.

Different choices of H(k,p) = different strategies for often addressing

______________
Linear probing
If a position is taken, try the next one:
H(k,0) = h(k), H(k,p) = (H(k,p-1) + 1) mod m. 
- wrap around to 0 when reach the end of the hash table

________
Deletion
If we simply delete then when we reach an empty bucket, cannot be sure that k is not somewhere else(ie. this empty bucket was occupied when k was inserted).

In practice: add a special placeholder "deleted" to distinguish buckets that were never used from the one that once held a value



************************************
014-Mar4
___________________________
Open Addressing (continued)
________
Deletion

If we simply delete then when we reach an empty bucket, cannot be sure that k is not somewhere else(ie. this empty bucket was occupied when k was inserted).

In practice: add a special placeholder "deleted" to distinguish buckets that were never used from the one that once held a value
- search will probe through deleted keys 
- deleted fields can be reused on successful inserts

Advantage: constant time delete
Disadvantage: eventually all of the free spots will be deleted => unsuccessful search O(m)
	      
If too many deleted keys, destroy and refill the table
_____
Cost:
Successful search: 1/2 (1 - 1/(1-alpha))
Unsuccessful search: 1/2 (1 - 1/(1-alpha)^2)

Expected number of probes:
__________________________________________
	50%	2/3	75%	90%
------------------------------------------
hit	1.5	2.0	3.0	5.5
miss	2.5	5.0	8.5	55.5



___________________________
Problem: Primary Clustering
- sequences merge together to form longer sequences
-> increase in average search time
what to do about primary clustring?
idea: is to use two harsh function (this is called double-hash)



- < hash function tell us to go here
-
-
-
-
-
-
-
-
-
* acctual insert place

Note: a large sequnce between hash place and acutal insert place

___________________________________
open addressing with double hashing
use a second hash funciton to pick the probe sequence increment:
H(k,0) = h1(k),	H(k,p) = (H(k,p-1)+h2(k))
where h1 and h2 are hash functions.
h1(k), h1(k)+h2(k), h1(k)+2*h2(k), h1(k)+3*h2(k),..... (all mod m)

potiential problem:
eg. m=10, h1(k)=3, h2(k)=2 for some key

prob sequence: 3, 5, 7, 9, 1, 3
(3, 3+2, 3+4, 3+6, 3+8(mod 10), 3+10(mod 10))

To prevent this: need h2(k) and m are relatively prime for all k.
There's an esay way to do this: make m prime, and h2(k) must be non-zero.

_____________
Cost analysis:
Assumption: h1 and h2 are both good harsh functions and behave indenpendently of each other.
	   In particular, the probability that H(k,p) hits an occupied position is n/m = alpha.

Unsuccessful search: U(n) = probablity(1st probed slot is empty)*1 + probability(1st full, 2nd is empty)*2 + .....
 		          = (1-alpha)*1 + alpha(1-alpha)*2 + alpha^2*(1-alpha)*3 + ....
 		          = 1 - alpha + 2alpha - 2alpha^2 + 3alpha^2 -3alpha^3 + 4alpha^3 + ....
		          = 1 + alpha + alpha^2 + alpha^3 ....
  		          = 1 / (1 - alpha) (since alpha < 1)
			  = 1/ (1- n/m)

Successful search: # of probes to look up a key = # of probes taken when the key was inserted

An insert of the n-th element was an unsuccessful search on n-1 elements before the n-th was inserted

So, successful search: S(n) = 1/n (U(0) + U(1) + U(2) + .... + U(n-1))
S(n) = 1/n * sum_{i=0}^{n-1} U(i)
= m/n * sum_{i=0}^{n-1} 1/(m-i) = m/n (Harmony(m)-Harmony(m-n)) approx= m/n (ln(m) - ln(m-n)) = 1/alpha * ln(1/(1-alpha))
        ^^^^^^^^^^^^^^^^^^^^^^^
	 harmonic sequence 
         1/m + 1/(m-1) + 1/(m-2) +  ...


Expected number of probes:
______________________________________
	50%	2/3	75%	90%
--------------------------------------
hit	1.4	1.6	1.8	2.6
miss	1.5	2.0	3.0	5.5



______________
Cuckoo hashing
Each (k,v) in either h1(k) or h2(k)
seach/delete: O(1) worst -case
Always insert in h1(k)
Expect cost for insert is : alpha/(1-2alpha)^2


Hashing Review:
________
Chaining
expect cost of hit: 1 + alpha/2
expect cost of miss: 1 + alpha
easy to implement
deletion is easy
dictionary can be larget than the table 
extra space to store SLL

______________
Linear Probing
prob h(k), h(k)+1, h(k)+2.....
primary clustring problem
expect cost of hit: 1/2 (1+1/(1-alpha))
expect cost of miss: 1/2 (1+1/(1-apha)^2)
Table may get too full -> rehash to large table

________________
Double - hashing
Use 2 functions h1 ahd h2
expect cost of hit: 1/2 ln(1/(1-alpha))
expect cost of miss: 1/(1-alpha)


Big-Theta cost of each operation (a reprensets alpha):
__________________________________________________________________
			search		insert		delete
-------------------------------------------------------------------
Linear Probing		1/(1-a)^2	1/(1-a)^2	1/(1-a)
-------------------------------------------------------------------
Double Hashing		1/(1-a)		1/(1-a)		1/a*log(1/(1-a))
-------------------------------------------------------------------
Cukooo Hashing		1		a/(1-2a)^2	1


__________________
Extendible hashing
what is hash table doesn't fit in main memory?
If need to access disk, ideally make only one access.
Idea: use a tree of height 1 / Alternatie to B-tree
        (two-level hash table)
directory("root node") is in internal memory
-> contains a hash table of size 2^d
-> d is called order
each directory entry points to a block in external memory
-> each block contains at most S entries sorted by hash values.
(records could be arranged as hash tables, tress, lists, etc ...)

hash h: {key} -> 0 ... 2^L -1, for some L>0,
hash h values are strings of bits of length L
eg.
hashes: 
00001	000 --->[00001]
01001	001 ---/
01010	010---->[01001
01110	011----> 01010]
10110	100
11100	101	[01110]
	110	[10110
	111	 11100]



 
************************************
015-Mar6
L - Length of hash values
d - order of directory (d <= L)
S - size of a block
Kb - local depth of block B (# of bits in keys hashes prefix that are significant for hashing)
Nb - size of block B (Nb <= S)

_________
Searching
1. Hash h(k) - a binary string of length L
2. Use first d bits to index into directory
3. Scan the corresponding block
   (eg. with Binary Search)
Cost: 1 disk access (CPU time: O(log S))

__________
Insearting
Search for the correct block B:
(3 scenarios of the following will happen):
case1: Has room in Block B 
	-> add(k,v)
case2: No room, and Kb < d. 
	-> split the block and redistribute the keys (might be multiple times)
	-> increment Kb of both blocks
	-> add(k,v)
case3: No room, and Kb = d
	-> double the block size (d = d+1)
	-> split the block, increment Kb
	-> add(k,v)

Example:
L = 5, S = 2

insert 01001 into an empty extensible hash table: d= 0
d = 0
[]----->[0| 01001]


insert 00001
d = 0
[]------>[0| 01001
	    00001]


insert 01110
d=0
[]------>[0| 01001
	    00001
	    01110] (no room, need to split, but Kb = d) 
            ``````
-> we double the directory:
d= 1
[0] ------> [0| 01001
[1] ------>     00001
	 	01110]

to 
[0] ------->[1| 01001
                00001
		01110]
[1]---------> [1|]

need to split again, d = Kb
[00]----------->[1| 01001
[01]----------->    00001
		    01110]
[10]----------->[1|
[11]----------->	]



[00]----------->[2| 01001]
[01]----------->[2| 00001
		    01110]
[10]----------->[1|
[11]----------->	]


insert 11100 and 10110 (both case 1)

[00]----------->[2| 01001]
[01]----------->[2| 00001
		    01110]
[10]----------->[1| 11100
[11]----------->    10110]


insert 01010 11101 01011 and 00000

[0000]----------->	[2| 00001
[0001]----------->	    00000]
[0010]----------->
[0011]----------->


[0100]----------->	[4| 01001]

[0101]----------->      [4| 01011
	   		    01010]

[0110]----------->  	[3| 0110]
[0111]----------->

[1000]-----------> 	[2| 10110]
[1001]----------->
[1010]----------->
[1011]----------->

[1100]----------->  	[2| 11100
[1101]----------->	    11101]
[1110]----------->
[1111]----------->



_________
Deletion: (reverse of insert)
Goal: keep directory as small as possible
-> merge with "buddy" if possible
	("buddy" is a block with same local depth and agress on first Kb -1 bits)
-> decrement Kb if the merge is possible
-> shrink the directory if possible

eg.

[0000]----------->	[2| 00001
[0001]----------->	    00000]
[0010]----------->
[0011]----------->


[0100]----------->	[4| 01001]

[0101]----------->      [4| 01011
	   		    01010]

[0110]----------->  	[3| 0110]
[0111]----------->

[1000]-----------> 	[2| 10110]
[1001]----------->
[1010]----------->
[1011]----------->

[1100]----------->  	[2| 11100
[1101]----------->	    11101]
[1110]----------->
[1111]----------->

now: delete 01011

in 
[0100]----------->	[4| 01001]

[0101]----------->      [4| 01011
	   		    01010]
first remove: 01011

[0100]----------->	[4| 01001]

[0101]----------->      [4|
	   		    01010]

we shrink it:

[0100]----------->	[3| 01001
[0101]----------->     	    01010]




That is:

[0000]----------->	[2| 00001
[0001]----------->	    00000]
[0010]----------->
[0011]----------->

[0100]----------->	[3| 01001
[0101]----------->     	    01010]


[0110]----------->  	[3| 0110]
[0111]----------->

[1000]-----------> 	[2| 10110]
[1001]----------->
[1010]----------->
[1011]----------->

[1100]----------->  	[2| 11100
[1101]----------->	    11101]
[1110]----------->
[1111]----------->



Example2:

[000]----------->	[2| 00001
[001]----------->	    00000]

[010]-----------> 	[3| 01010
			    01001]

[011]----------->	[3| 01110]

[100]----------->	[2| 10110]
[101]----------->      

[110]----------->  	[2| 11100
[111]----------->	    11101]

delete 01010:


[000]----------->	[2| 00001
[001]----------->	    00000]

[010]-----------> 	[2| 01001]
[011]----------->	    01110]

[100]----------->	[2| 10110]
[101]----------->      

[110]----------->  	[2| 11100
[111]----------->	    11101]

Note: all other blocks have remained untouched just update links from the directory.
Now shrink them:

[00]----------->	[2| 00001
			    00000]

[01]-----------> 	[2| 01001]
	        	    01110]

[10]----------->	[2| 10110]
      
[11]----------->  	[2| 11100
	         	    11101]


_______________________
Cost for insert/delete:
Cpu: Theta(S) (without directory grow/shrink directory size change to Theta(2^d))
	(also we want the L and d be relatively small)
Disk accesses: 1, or at most 2 (if merge)

Space Efficiency:
Does directory gets too large?
Will there be too many pages?
Will there be many empty or almost empty pages?

Eg. we cannot merge the two pages of depth 4, because the first one is already full.
We waste a lot of space of the other page and the directorys.

0000 --------------->	[4| 00000
			    00010]
0001 --------------->   [4| 00010]

...   			[3| ]
			[2| ]
			[1| ]



But, the above example is not always the case.
Assumption: bit patterns of key hashes are uniformed distributed.
Expect number of pages is approx = 1.44 h/s (not sure whether h or n or whatever!)
	-> pages are about 69% full
Expect direcory size O(n^(1+1/S) /S)


_____________
Sorted Arrays
if data is stored in a sorted array: insert/remove Theta(n)
				     lookup 	   Theta(log n)

Question: when you search a phone book, is binary search what you do?
eg. B or C -> search near the front
    X or y -> search near the back
Can we do this when we search array?



____________________
Interpolation Search (based on linear interpolation)
(see slides)
Idea: use the value of the key to GUESS its location

The expected time is (assume uniformly distributed):
T(n) = Theta(1) + T(sqrt(n))
T(n) = Theta(log log n)

Eg. using interpolation search on 4 billion entries:
expect runtime (assume uniformly distributed):
log log 4*10^9 aprox = 5 that's so fast!


But consider an array (worst case):

A = [10^0 10^1 10^2 .... 10^9]
seach for 10^7, l=0, r=9
m = 0 + floor(1/10^2 * 9) = 0
l =1, r=9
m = 1 + floor((10^7 -10^1)/(10^9 -10^1)) = 1 -> l = 1 , r =9

-> middle goes up by 1, each time O(n)







************************************
016-Mar11
_____________
Gallop Search (sometimes we cannot see the end of the date. eg date stream, a huge file,etc)
What if we can't see the end of the array? 
Idea: "Gallop" through to find a valid range for Binary Search
We are searching for k in A[0,...], K could be anywhere in A

Example:

A = [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16]
     < <   <       <            k       >=
	           | the range we search |

int i=0
k > A[0] => A[1....], i =2i+1 -> i=1
k > A[1] => A[2....], i =2i+1 -> i=3
k > A[3] => A[4....], i =2i+1 -> i=7
k > A[7] => A[8....], i =2i+1 -> i=15
k <= A[15] => A[8...15] -> perform binary search in the interval

O(log m) comparison (m: the location of k in A)



______________________
Self-Organizing Search
Unsorted Linked Lists
Model of Comparisons:
-search at position: 1, 2, 3,  ...
 (until found, or unsuccessful)
-only count data comparasions
-all unsuccessful searches take n comparasions
-> only consider successful searches

Expected case:
Suppose the list contains n keys: k1, k2, ..., kn, and the probability to search for a key k(i) is P(i)
Sum_{i=1}^{n}P(i) = 1 
The expected case of successful search when probability of searching for any key k(i) is 1/n (uniform distribution)
1/n*1 +1/n*2 + 1/n*3 + ... +1/n*n
= 1/n n(n+1)/2 = (n+1)/2

If there is no uniform distribution, what arrangement of the list minimize the expected # of comparasions to find an element?
-> store nodes in decreasing probablity of search, so let's assume P1>=P2>=P3....>P(n)
Optimal ordering: k1, k2, ..., k(n)

Let C_opt be the expected number of comparasions under optimal ordering 
C_opt = sum_{i=1}^{n} i*P(i)

But we don't necessarily know the optimal distribution in advance, and it may change.

-> we can approximate the optimal distribution by rearranging the list after every lookup
-> by using a self-organizing heuristic

_______________________
More-to-front heuristic
After searching for key k(i), put k(i) at the front of the list:
Example: 
initial list: 1 2 3 4 5 6
search(3): 3 1 2 3 5 6
search(4): 4 3 1 2 5 6
search(6): 6 4 3 1 2 5


Let C_MTF be the expected cost of a lookup under more-to-front heuristic.
Theorem: C_MTF <= 2* C_OPT -> 2-competitive 
Proof:
C_MTF = sum_{i=1}^{n} P(i)*(expected search cost of k(i))
      = sum_{i=1}^{n} P(i)*(1 + # of keys before k(i))
    
Note: what is the number if keys before k(i)?
For a given j!=i, what is the probablity that k(i) was searched for more recently than k(i)?

Probability( k(j) searched more recently than k(i) ) = P(j) / (P(i) + P(j))
							      ^^^^^^^^^^^^^
							      The probability of searching for k(i) or k(j)

Eexpected search of k(i) is:
1 + sum_{i!=j}  P(j) / (P(i) + P(j))

C_MTF = sum_{i=1}^{n} (P(i)* (1+ sum_{i!=j} P(j) / (P(i) + P(j)) ))
      = sum_{i=1}^{n} P(i) + sum_{i=1}^{n} ( sum_{i!=j} P(j)P(i) / (P(i) + P(j)) )
      = 1 + 2*sum_{i=1}^{n} ( sum_{i<j} P(j)P(i) / (P(i) + P(j)) )   (use i<j instead of i!=j, pr * 2)
      = 1 + 2*sum_{i=1}^{n] P(i) (sum_{i<j} P(j) / (P(i) + P(j)))
since P(j) / (P(i) + P(j)) <= 1
      <= 1 + 2sum_{i=1}^{n} P(i)(i-1) 
      = 1 + 2C_OPT -2
      = 2C_OPT - 1

_______________
Note about MTF:
bad in arrays
affected by rare lookups
responds quickly to a change in distribution
works well in practice

___________________
Transpose Heuristic
After seaching for key k(i), swap it one position closer to the begining of the list 
(with the element appearing right before k(i))
Example: 
inital list: 1 2 3 4 5 6
search(3): 1 3 2 4 5 6
search(4): 1 3 4 2 5 6
search(6): 1 3 4 2 6 5

Note: slow to a steady state good in an array
      unaffected by rare lookups


What about a sorted linked list?

[ ] -> [ ] -> [   ] -> [ ] -> [ ]
 |             ^ v             ^
 |             | |             |
 --------------- ---------------

Can't jump around the list, so Binary search won't work.
What if we add an "express lane",
pointers from 0->2->4.....
Add pointers 0->4->8->12->.....
Call the pointers from 
0->1->2->3	level 0 pointers
0->2->4->8	level 1 pointers
0->.....	level i pointers
(i = 0 to log n)
lv3    	[]------------------------------------[47]
lv2	[]----------------[17]----------------[47]
lv1	[]------[ 5]------[17]------[31]------[47]
lv0	[]-[ 2]-[ 5]-[11]-[17]-[23]-[31]-[41]-[47]

This is called a "skip-list" (discovered in 1989)
Total space used for extra pointers:
For 1/2 the nodes	->0 extra pointers
For 1/4	the nodes	->1 extra pointers
For 1/8 the nodes	->2 extra pointers
...
Total extra pointers: sum_{i=1}^{infty} n/2^(i+1) * i  = n /2 * 2 = n

Extra space cost is linear, each node has 1 extra pointer on average.

Note: how everything is perfectly symmetic -> impossible to maintain ->
every insert/delete means restructuring the entire list.
Instead of an ideal skip list we'll build a skip list that's good enough

_________
Intuition
Level i contains approximately twice as many items as level i + 1

When a key is inserted, choose its level at random :
Its level is 0 with probability 1/2
Its level is 1 with probability 1/4
Its level is ....
Its level is i with probability 1/2^{i+1}

******************************************
level=0, coin = flip a coin
while coin == H
   level++
   coin = flip a coin
******************************************







************************************
017-Mar13
Skip-search will return the largest element smaller than the search value.

There's a mistakes on slide 15.

(s3,-infity)
(s2,-infity)
(s1,37)
(s0,44)


Expected space usage: O(n)
Expected height : O(log n)

skip-search: O(log n)
skip-insert: O(log n)
skip-delete: O(log n)


__________________________
Controlling Height Options
1. Force h<=c, for sone constant c - problem as n -> inifty
2. Force h<=f(n), f(n) = 3log n is good -> the likelyhood of having a skip lsit of height 3logn is 1/n^2
3. Allows h->infity, not likely to get too high.

_______________
Space Analysis:
Expected keys at level i:
sum_{i=1}^{n} 1/2^i = n / 2^i

Expected # of keys in a skip list of height h
sum_{i=0}^{h} n/2^i = n (1 - 0.5^(n+1)) /(1-0.5) = 2n - n/2^h  ~= 2n



_______________
Time analysis:
Worst case: O(n+h)
eg. a linked list-like skiplist or a "very-tall" linked list
-> no "bad" input will casue this probabilily structured skip lsit

________
Theorem
search (insert/delete) is O(log n) in a skip list of size n. (always hold no matter how worse the skip-list)

____
Idea
Backwards analysis: consider the reverse of the path you took to find k, which consistes of "up" and "lefts".
Observation: At any position P on the path to find k, you will always move up is possible on the reverse walk.
(you always "skip" to an element tower)
At any position P(Tower T, level i), you will to left if the height of T is i.

Let C(k) be the expected path length to go up k levels.
C(0) = 0
The probability of case1 is 1/2, Prbablity of case 2 = 1/2
c(k) = 1/2 * c(k-1) + 1/2 * c(k) + 1
c(k) = 2 + c(k-1), c(k) = 2k.

since O(log n) levels, we have O(log n) cost of total path.






__________________________________
Module7 Multi-dimension data

____________
Range search

one-dimension: given a dictionary, and key values L and U, find all keys k such that L<=k<=U.

muti-dimension range search:
First option: Quad trees
The root of a quadtree corresponds to a square that contains all the points 

Every non-leaf node has 4 children:
NW,NE,SE,SW: ordered, possibly-empty
and divides the space equally into quadrants
      
       |
NW     | NE
--------------
 SW    |  SE
       |
Do it recursively

Each leaf node contains at most one key.
-> if a candidate leaf contains more than 1 point split until the points are separated.


In 3 dimensions, build an octtreee -> sub-divide space into 8 octants .
in k-dimension, 2^k trees


  -------/|
 /  /   / |
 ------/| |
/  /  / | | 
------- |/|
|  |  |/| |  
------- |/
|  |  | /
-------/

Let P be a set of points inside a squeare 
[x1, x2] X [y1,y2]

If |p| > 1, recurse into l1 subquadrants 
x_mid = 1/2*(x1+x2)
y_mid = 1/2*(y1+y2)
If p in p on x_mid then P is in NW or SW
If p in p on y_mid then p is in SW or SE

How tall will the quadtree be?
Depends on the distribution of points.
If spread out -> short tree, fast searching
concentrated points and nodes -> tall tree


_____
Lemma
Depth of a quadtree is at most log(s/c) + 3/2
s is the side length of initial square, c is the minimum distance between any 2 points:

ProofL At depth i, size length of a quare is s/2^i
max distance between a points is
squrt((s/2^i)^2 + (s/2^i)^2) = sqrt(2) * s/2^i



__________
kd-trees
kd-tree idae: split the points into two (roughly) equal subsets
A blanced binary tree.




************************************
018-Mar18
kd-trees:
Idea: split on x-coordinates and on y-coordinates
A(6,6)
B(1,2)
C(3,4)
			A
		      /
		     B
		      \
		     	C

D(4,1)
E(5,7)
F(7,5)
G(2,3)
			A
		      /	 \
		     B    F
		    / \
		   D   C
		      / \
		     G   E

	
			
			A		A(6,6)                
	          x<=6/	 \ x>6		  ^
		     B    F		B(1,2)
		y<=2/ \ y>2		    ^
		   D   C		C(3,4)
		  x<=3/ \ x>3		  ^
		     G   E		


(see notes for more details)

Range search:
The only nodes traversed are:
S1: ones that coorespond to a point in R 
   |S1| = k
S2: ones that correspond to a point not in R but whose region intersects with R
   |S2| <= # of nodes in the tree C such that C region is intersected by an edge in R.
Let L be a fixed vertical line (an edge of R).
Let Q(n) be # of nodes c in a kd-tree with n points such that c.region is intersected by L.

                   	|
			|
	       *	|
			|
			| 
			^
		 	L
Q(1) = 1


	|            	|
	|	*	|
	*		|
	|		|
	|		| 
	|		^
       l1	 	L


After line l1, L intersect exactly ine of T_L.region or T_R.region.


~n/2 pts|  ~n/2 pts
 	|
	|       *     	|
--------|---------------|-----------------
	*		|
	|		|
	|		| 
	|		^
       l1	 	L

Q(n) = O(1) + 2Q(n/4)
=> Q(n) = O(sqrt(n))



_____________________________
Orthogonal/ rectangular seach
Orthogonal/ rectangular Query on a kd-tree with n points takes O(sqrt(n)+k) time, where k is # of points in the range.
- But the algorithm will work for arbitrary range.

kd-tree
Storage: O(n)
Construction time: O(nlog n)
Range query time: O(n^(1-1/d)+k) d is considered to be a constant



__________
Range tree
For a 2-dimensional range search:
construct a BST using x-coordinate as the key.

				.(40,*)
			     /     \
 		      (35,*).	    .(43,*)
			  /   \   /   \
		   (30,*).     .  .    . (45,*)
			/ (37,*)(41,*)/ 
		 (25,*).	     . (44,*)	
				      	

Boundary nodes in side nodes (30,*)
outside nodes (25,*)

For each node store a pointer to a BST on all the points contained by that node, use y-coord ad the key.

Then let's say nodes
	b1 b2, ...
	i1,i2,...
        are reported fro, the range seach in the first tree.
For each "inside" node,perform a range search on y. -> O(log n) * O(log n + ki)
And the sum of ki will be k.
So, the total runtime = O((log(n))^2 + k)


1. Build the initial BST tree T (sorted by x-coord)
For T.root:
   build the BST on y-coord
   recurse on the left node
   recurse on the right node

T(n) = 2T(n/2) + nlog n
=> T(n) = n(log n)^2

If pre-sort points by the y-coord :
T(n) = 2T(n/2) + n
=> T(n) = nlog n



Space complexity:
Consider a point P:
There is a unique path that contains P. P will only be stored in the associated trees for the nodes on the path from the root to P.
-> O(log n) such trees

Each of n points is stored in O(log n). associated structures -> O(nlog n) space
************************************
019-Mar20
____
Trie (Radix Tree)
(Trie comes from retrieval, pronounced "try")

struct of trie:
1. only store key on leaves
2. 0 go left
3. 1 go right


prefix-free: no key is a prfix of another key
eg.
01011
010


_____________
Tries: search

If we get stuck on the internal node or we cannot proceed any more. Then the seach is unsuccessful.



_____________
Tries insert
 0 1 2 3 4 5 6
------------------------
 0 0 1 1 0 1
 0 0 1 0 1 0
 0 0 1 0 1 1 0
 1 1 0 0


				0
			       / \
			      3   1100
          		     / \   
			    5 00110 
  	                   / \
       	              001010 0010110


now insert(1110):
        _
	1110
	1100
	  ^
  diff on 2

				   0
			       /      \
			      3        2  
          		     / \      / \
			    5 00110 1100 1110
  	                   / \
       	              001010 0010110
now insert(001000):
	_  _ _            
	001000
	001010 the node agree on digit 0 and 3
	    ^
    diff on 4 
So the insert should happen between 3 and 5.

				   0
			       /      \
			      3        2  
          		     / \      / \
			    4 00110 1100 1110
			   / \
	  	      001000  5
  	                     / \
       	                001010 0010110
		



________________
Pattern matching

T = *************
     *****

|T| = n
|p| = m

worst case: m comparisons for eah guess n-m+1 guesses
O((n-m+1)*m) 

eg. T = aaaaaaaaaaaaaaaaaab
    P = aaaab




************************************
020-Mar25
________________________
Knuth-morris-Pratt (KMP)

T = PROGRAM........
P = PROGRAD

Case1: T[i] = P[j]         j = m -1
               i
T = .... bacbababac.....
P =        cbaba
               j
return i-j, the first occurrence of P in T.


Case2: T[i] = P[j],  j < m-1
T = ...... bacbababac........
P = .........cbabac 
-> keep matching, i++ j++


Case3: T[i] != P[j], j=0
T = cbabab
P =   bab
i++, j remains 0.


Case4: T[i] != P[j], j>0
T = ablablac
P =  blablab


In general:
		     T[k]		     T[k+j]
		       v                      v
T = .................. ABC ..................JKL.............
P =                    ABC ..................JXY
		       ^	 	      ^
		       P[0]      	   mismatch


Let d = # of slots we can safely shift to the right 0<d<=i.
			    mismatch occurs here
				v
T = ............................|...................
P =            .................|......
			........|...............

               |-d slot-|

		t[k]		     t[k+j]
		v                     v
T = ........................................................
P =             ............................
		^ 	^	      ^
               P[0]	P[d]	      P[j]	
			............................
			^	      ^
			P[0]	      P[j-d]
		|-  d  -|


-> P[0] = P[d]
   P[1] = P[d+1]
	...
   P[j-d-1] = P[j-1]

d should be as small as possible.

So we can write a function that takes in P(the pattern) and s (location of mismatch) and return d.

_________________
KMP Failure Array
We want the longest suffix in the pattern P that is also a pre-fix of P.

eg. P = abcaba
         abcab      NO
          abca      NO
           abc      NO
            ab      NO
             a      YES


************************************
021-Mar27
___________________________
Boyer-Moore String Matching
- Suitable when P is relatively long, and the alphabet is relatively large.
(Apply both bad character and good sufix (KMP algorithm) and at each step use the better shift (eg. shift 3 vs shift 5) )


Key difference with KMP:
P is compared with T from the right. 
(Goal reamins to find the first match index.)

T = ............................
	->shift the pattern this way
P = .............
	<- compare this way

eg. 
T = PROGRAMMING
P = PROD
       ^
       x    G is not in P

        PROD
	   ^
	   x    M is not in p
	    PROD
	       ^
               out of bound

-> Done, this pattern is not found.

Key-advantage: do not even nessarily head to look at all of the chars in T.

eg.

T = PROGRAMMING
P = GREP
       ^
       x  G is in P !

       GREP
          ^
          x M not in P

           GREP
	      ^
              x  G in P!
              GREP
	       ^
               Out of range.


eg.
T = PROGRAMMING
P = RURAL
        ^
        X  R is in P!

      RURAL


All of above is called the bad character heuristic.


eg. 
T = WXYZGABCDABC
P = GABCDABC
        ^
        X
Note P may contains some prefix which is also a suffix in P. For an eariler substring in P that matches the current matched suffix,
shift P such that this earlier match lies under the matchd character in P.

        GABCDABC     

    

Sometimes bad char will do something stupid.

eg.
T = AURALAMMING
P = RURAL
    ^
 RURAL  ?

    
The idea is to apply "bad char" and "good sufix" algorithm together.

P = odetofood
T = ilikefoodfrommexico 
    odetofood
        ^
bc:   odetofood
gs:        odetofood

I perfer the gs one!


We calucate the last occurence array.


Now we also need to compute the suffix skip array.

eg.
suffix skip array

i = 0 1 2 3 4 5 6 7 8 9 10
P = A B R A C A D A B R A
S =-7-6-5-4-3-2-1-3-2 6 9
		    ^
		this one is difference, since we cannot get such ?[!b]+"RA", we try to find the P such that that contains sufix of "RA" we get "A"
					       starts here!
			find this way	<-	  v
________________________________________________________
char1	substring-matching-suffix 	char2	suffix
--------------------------------------------------------
R					A	""
D					R	A
					B	RA
					A	BRA
					D	ABRA


Boyer-Morre algorithm 
worst case runtime O(n+|sigma|)
For English text, the average case for re-align is 25%.


____________
Suffix Trees
what if we want to search for multiple patterns in the same text -> preprocesss the text

___________
Observation
* P is a substring of T iff P is a prefix of (a suffix of T)

........................[P...............]
                        suffix of T
                      P...............
                    P is in prefix of (suffix of T)

* it's easy to earch a trie for prefixes.

T = ABRACADABRA
    BRACADABRA
    RACADABRA
    ACADABRA
    CADABRA
    ADABRA
    DABRA
    ABRA
    BRA
    RA
    A

don't need to add these in the trie as they are prefixes of words already in the tri
We can find pattern P in T, in O(m) time -> very fast.
(+ time to build the trie)

				        0
			    A/     |B        \C     \D   \R
			   1    BRACADABRA    \      \    RACADABRA 
			B/ | \D                \    DABRA
                      	/  |  \               CADABRA
              ABRACADABRA  | ADABRA
                        ACADABRA

   eg. search for ADAB follow A->D check if ADAB is a prefix of the key








 
************************************
022-Apr1
_____________________
Module 9: Compression

A prefix-free dictionary of binary string(codes)
eg.

0100
010010

Then, we cannot decode this, 01001000100

ASCII is a fixed-length code, and every key string in this has the smae length 7 bits.

encodeing:

ASCii :   0xxxxxxx
	   7bits
Unicode Encoding:
110....  10......			2bytes
1110...  10.....   10.....		3bytes
11110..  10.....   10.....  10.....	4bytes


___________________
Run-Length Encoding
11100000001111111111111110010000000100001
we have a lot of 0's and 1's. We call it a "run".
Normally we will have a long run because of "Locality".

But how can we encode a rin length k in binary?

(1) pre-fix encoding of integer k
k in binary
eg. 45: 101101

(2) let |k| denote the length of binary representation
eg. |k| = 6 (101101 has 6 bits)

(3) write down the unary representation of |k|
6: 0000001
eg. Unary representation
3: 0001
1: 01
0: 1
x: x zeros followed by 1

(4)Assuming |k| >=1, then unary representation end with 01
delete the last "01" from the encoding

0000001
     ``
remove it, let's save 2 bits, 'cause |k| >=1

(5) Run Length Encoding
00000
=> 
00000101101


eg. 1
B: 1
|k| = 1
01
=>
1

eg. 3
B: 11
|k| = 2
001
=>
011


Noramlly, when k > 6. we start saving space.

************************************
023-Apr3
_________________________
Lempel-Ziv-Welch encoding
Observation: some sequences of characters occur often, eg. the, and , is ,ion, of the ......
Given these a fixed length bit sequence. 

Idea: have a dictionary of codes assigned so far
- try to grab as big of a chunk of the string as we can find in the dictionary
- add new entries as we go
-> an adaptive encoding algorithm

Initial dictionary: one entry for every single character
How many bits to use to encode each entry? k = 12 is typical approx= 4000 entries (2^k = 4096) (k=12 is the author choose, larger than that the compression is becoming larger)


__________________
Encoding Algorithm
Init D with single chars.
S = remove first char of input
output code(s)


while input remains
   t = s
   s = remove longest prefix of input text that is in D
   output(s)
   c = first character of strings
   add tx in D, with next free code number


__________________
Decoding Algorithm
Init D with single chars
s = decode(remove the first code number from c)
output s
while input code remains
  t = s
  s = decode(remove the first code nuber from input codes)
  output s
  c = first character of s
  insert tc into D, with the next available 

What if we run out of dictionary space
options:
1. Do nothing
-> bad if data changes structure
2. Erase the dictionary(but not the single char set ) and start over
-> temporary low compression until the dictionary is rebuild
3. overwrite old dictionary entries 
e.g. discard least frequently used -> could be expensive
4. grow the dictionary

Dictionary implementation:
-For compression:
    need to map strings to codes 
    -> trie is ideal

-For decompression:
    map codes to strings
    -> array


_________________________
Burrows-Wheeler Transform
What kinds of string are better compressible?
- lots of repeated characters
What can we do to a string to produce lots of repeated characters?
-> sort the charactors
But: we can't undo this!
eg.
abracadabra
=> sort
aaaaabbcdrr

A more careful approach is the BWT.
- pemutes the characters and is reversible
eg.
Consider a string: ABACAB
We need an end marker,say $.
Whatever we choose, it cannot appear in the text. ABACAB$

ABACAB$
BACAB$A
ACAB$AB
CAB$ABA
AB$ABAC
B$ABACA
$ABACAB

We sort the rotations (lexicographically)
In our case $ comes before A but this is not essential: 

$ABACAB
AB$ABAC
ABACAB$
ABAB$AB
B$ABACA
BACAB$A
CAB$ABA
      ^
     last column
c = BC$BAAA (the last column)
        ```
        we have a block of A

string is NOT shorter but there are repeated characters.
Two big questions:
1. Is this reversible and how?
2. Why does it work?


C = BC$BAAA

B
C
$
B
A
A
A

Now we sort it:

$	B
A	C
A	$
A	B
B	A
B	A
C	A

Now we know that:

$	B$
A	CA
A	$A
A	BA
B	AB
B	AB
C	AC

=> sort

$A
AB
AB
AC
B$
BA
CA

=> prepend last col
B	$A
C	AB
$	AB
B	AC
A	B$
A	BA
A	CA

=> 
B$A
CAB
$AB
BAC
AB$
ABA
ACA

=> now sort the first col again , prepend , then sort ,... continue the procedure over and over until we get 7 colums in this example


Why this works?
Consider English Text:
Sequences like the, and ,ing are very common.
Under retation and sorting:
HE......T
HE......T
NE......A
ND......A
        ^
     repeat
eg.

bubba$ 
ubba$b
bba$bu
ba$bub
a$bubb
$bubba

=> sort

$bubba
a$bubb
ba$bub
bba$bu
bubba$
ubba$b
     ^
     C
C = abbu$b

  sort=>
a,0	$,4
b,1	a,0
b,2	b,1
u,3	b,2
$,4	b,5
b,5	u,3

Recover from$ 		original input: 
$,4	b,5		b
b,5	u,3		bu
u,3	b,2		bub
b,2	b,1		bubb
b,1	a,0		bubba
a,0	$,4		bubba$



